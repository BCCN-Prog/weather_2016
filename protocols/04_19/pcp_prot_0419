Protocol 19.04.

Topics:
- Recapitulation of the project outline
- Brainstorming for long term goals
- Definition of prephase and first work phase
- First research regarding primary sources/features
- Groups for first phase, start group work


Planning

Long-term goals:
With scraping:
- Prediction vs. Actual
	- deviations wrt location/time (week/year)
- Quality/bias heatmap
- Reverse-engineer what the rain probability percentage means
- Unbiased forecast
Without scraping:
- Climate change
- Historic data analysis (also e.g. link with health data)
- Weather sayings, regional truth

Prephase: 
- Data overview: 
	Which primary sources exist?
	Which features are important?
	What datatypes exist?
- Method overview:
	Code from year above?
	Scraping tools

First Phase:
Workgroup Composition
- Scraping
	Denis
	Jan 
	Claudia
	Claus
	Greg
	Alicja
	Ines
- Historic Data
	Max
	Wolfi
	Rahul
	Filip
- Server Setup and Data Representation
	Eren 
	Georg

Results:

Scales and Variables:
Historical:
	Locations: all stations
	Variables: See git
	Timescales: Daily, Hourly(at least for predictions)

Predicitions:
	Locations:
	if possible: 400 Stations, as similar as possible for the historical data
	--> limiting factors? --> how often can you scratch?
	Plan B: 20 most important stations (List by Eren)
	Variables: Same as historical, see github
	Timescales: 24 hrs prediction, for all hours until then or exactly for the time 	point in 1 day, middle: 3 days, long: 10 days

Scraping group:
	Format .json
	Parameters: Website, Location, Timescale, feature variable, value
	Separation of tasks: Downloading (get HTML files), data extraction from the files



Things to figure out:
- How to treat missing data?
- When to scrape?
- Historic group data format? --> Save data redundantly?

Goals for next time:
- Have downloading script running on server

