{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.007999996247236e-05\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import timeit  \n",
    "\n",
    "def get_hist_and_rec(station_number):\n",
    "    #create \"artificial\" wildcard path for historical data. For every station imaginable. \n",
    "    histpath_temp = './ftp-cdc.dwd.de/pub/CDC/observations_germany/climate/daily/kl/historical/produkt_klima_Tageswerte_*'\n",
    "    histpath_temp += str(station_number).zfill(5)+'.txt'\n",
    "    #create \"artificial\" wildcard path for recent data. For the station we're looking at right now.       \n",
    "    recpath_temp = './ftp-cdc.dwd.de/pub/CDC/observations_germany/climate/daily/kl/recent/produkt_klima_Tageswerte_*'\n",
    "    recpath_temp += str(station_number).zfill(5)+'.txt'   \n",
    "\n",
    "    #check if that path actually exists. Globglob checks if the histpath file actually exists.\n",
    "    if len(glob.glob(histpath_temp)) != 0:\n",
    "        #if file exists, save the path as a string to \"histpath\" variable.\n",
    "        histpath = glob.glob(histpath_temp)[0]\n",
    "        hist_ = pd.read_table(histpath, sep=\";\", low_memory=False)\n",
    "        #is_hist = True\n",
    "\n",
    "    else:\n",
    "        #is_hist=False\n",
    "        hist_ = []\n",
    "\n",
    "        #check if recent data exists\n",
    "    if len(glob.glob(recpath_temp)) != 0:\n",
    "        recpath = glob.glob(recpath_temp)[0]\n",
    "        rec_ = pd.read_table(recpath, sep=\";\", low_memory=False)\n",
    "        #is_rec = True\n",
    "\n",
    "    else:\n",
    "        #is_rec = False\n",
    "        rec_ = []\n",
    "    return (hist_,rec_)\n",
    "\n",
    "\n",
    "def merge_hist_rec(hist,rec,interval_type,stationnumber):\n",
    "    \n",
    "    \n",
    "    if interval_type == 'daily':\n",
    "        if len(hist) != 0:\n",
    "            hist.columns = ['Stations_id', 'Date', 'Quality', 'Air_temperature', 'Steam_pressure', 'Cloudiness', 'Airpressure_stationsheight', 'relative_moisture', 'Air_speed', 'Air_temperature_max', 'Air_temperature_min', 'Soil_tem_min', 'Wind_speed_max', 'Rain', 'Rain_ind', 'Sunny_hours', 'Snow_height', 'eor']\n",
    "            hist = hist.ix[:len(hist)-2] #cut last line - it's empty\n",
    "            last_date = hist.Date[len(hist)-1] #extract last date of historical data\n",
    "            if len(rec) == 0:\n",
    "                complete_data = hist\n",
    "\n",
    "        if len(rec) != 0:\n",
    "            rec.columns = ['Stations_id', 'Date', 'Quality', 'Air_temperature', 'Steam_pressure', 'Cloudiness', 'Airpressure_stationsheight', 'relative_moisture', 'Air_speed', 'Air_temperature_max', 'Air_temperature_min', 'Soil_tem_min', 'Wind_speed_max', 'Rain', 'Rain_ind', 'Sunny_hours', 'Snow_height', 'eor']\n",
    "\n",
    "            if len(hist) != 0:\n",
    "                try:\n",
    "                    rec_starting_idx = (rec.loc[rec['Date'] == last_date].index.tolist()[0])+1\n",
    "                #if hist and rec do not overlap, we don't want to cut rec.\n",
    "                except IndexError:\n",
    "                    rec_starting_idx=1\n",
    "                #Assign rec_cut: it's the original rec, but starting from index specified above.\n",
    "                rec_cut = rec.ix[rec_starting_idx:]\n",
    "                combined = pd.concat([hist,rec_cut],ignore_index=True)\n",
    "                complete_data = combined\n",
    "                \n",
    "            else:\n",
    "                complete_data = rec\n",
    "        \n",
    "        \n",
    "        complete_data = complete_data.replace(-999, np.nan, regex=True)\n",
    "        #note: this replaces existing files.   \n",
    "        complete_data.to_csv('./clean_data/'+str(stationnumber)+'_'+interval_type+\".csv\")\n",
    "        \n",
    "\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "\n",
    "#for stationnumber in range(0,17000):\n",
    "#    (hist_out, rec_out) = get_hist_and_rec(stationnumber)\n",
    "#    if (len(hist_out) != 0) or (len(rec_out) != 0):\n",
    "        \n",
    "#        merge_hist_rec(hist_out,rec_out, 'daily', stationnumber)\n",
    "    \n",
    "\n",
    "stop = timeit.default_timer()\n",
    "\n",
    "print (stop - start) \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215.58342143699997\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import timeit  \n",
    "#STATIONS_ID; MESS_DATUM; QUALITAETS_NIVEAU; STRUKTUR_VERSION; LUFTTEMPERATUR;REL_FEUCHTE;eor\n",
    "foldernames = [\"air_temperature\",\"cloudiness\",\"precipitation\",\"pressure\",\"solar\",\"sun\",\"wind\"] #<- problem^^\n",
    "parameter_columnnames=[['Stations_id', 'Date', 'Quality', 'Structure_version', 'Air_temperature', 'Moisture', 'eor'],\n",
    "                      ['Stations_id', 'Date', 'Quality', 'Cloudiness','eor'],\n",
    "                      ['Stations_id', 'Date', 'Quality', 'Rain_fall_ind', 'Rain_height','Type_of_rain' ,'eor'],\n",
    "                      ['Stations_id', 'Date', 'Quality', 'Airpressure_reduced', 'Airpressure_station', 'eor'],\n",
    "                      ['Stations_id', 'Date', 'Quality', 'Sun_duration', 'DIFFUS_HIMMEL_KW_J', 'GLOBAL_KW_J','ATMOSPHAERE_LW_J','SONNENZENIT','MESS_DATUM_WOZ' ,'eor'],\n",
    "                      ['Stations_id', 'Date', 'Quality', 'Structure_version', 'STUNDENSUMME_SONNENSCHEIN', 'eor'],\n",
    "                      ['Stations_id', 'Date', 'Quality', 'Structure_version', 'WINDGESCHWINDIGKEIT','WINDRICHTUNG' ,'eor']]\n",
    "name_param_dict = {} #value must be missing or too much in the line of wind!\n",
    "for i in range(len(foldernames)):\n",
    "    name_param_dict[foldernames[i]] = parameter_columnnames[i]\n",
    "\n",
    "#hourly_filenames = [produkt_temp_Terminwerte_18930101_20151231_03987,\n",
    "#                    produkt_synop_Terminwerte_19490101_19500630_01260\n",
    "#                   produkt_synop_Terminwerte_19950901_19951023_03538\n",
    "#                   produkt_synop_Terminwerte_19490101_19500630_01260\n",
    "#                   solar: produkt_strahlung_Stundenwerte_19451231_20160331_03987\n",
    "#                   produkt_sonne_Terminwerte_18900101_20151231_01580\n",
    "#                   produkt_wind_Terminwerte_18930101_20151231_03987]\n",
    "\n",
    "#NIEDERSCHLAG_GEFALLEN_IND;NIEDERSCHLAGSHOEHE;NIEDERSCHLAGS\n",
    "#STATIONS_ID; MESS_DATUM; QUALITAETS_NIVEAU; LUFTDRUCK_REDUZIERT;LUFTDRUCK_STATIONSHOEHE;eor\n",
    "#STATIONS_ID; MESS_DATUM; QUALITAETS_NIVEAU; ERDBODENTEMPERATUR;MESS_TIEFE; ERDBODENTEMPERATUR;MESS_TIEFE; ERDBODENTEMPERATUR;MESS_TIEFE; ERDBODENTEMPERATUR;MESS_TIEFE; ERDBODENTEMPERATUR;MESS_TIEFE;eor\n",
    "#STATIONS_ID; MESS_DATUM; QUALITAETS_NIVEAU; SONNENSCHEINDAUER;DIFFUS_HIMMEL_KW_J;GLOBAL_KW_J;ATMOSPHAERE_LW_J;SONNENZENIT;MESS_DATUM_WOZ;eor\n",
    "#STATIONS_ID; MESS_DATUM; QUALITAETS_NIVEAU; STRUKTUR_VERSION; STUNDENSUMME_SONNENSCHEIN;eor\n",
    "#STATIONS_ID; MESS_DATUM; QUALITAETS_NIVEAU; STRUKTUR_VERSION; WINDGESCHWINDIGKEIT;WINDRICHTUNG;eor\n",
    "\n",
    "def get_hist_and_rec_hourly(station_number, parametertype):\n",
    "    if parametertype != \"solar\":\n",
    "        #create \"artificial\" wildcard path for historical data. For every station imaginable. \n",
    "        histpath_temp = './ftp-cdc.dwd.de/pub/CDC/observations_germany/climate/hourly/'\n",
    "        histpath_temp += parametertype+'/historical/produkt*'\n",
    "        histpath_temp += str(station_number).zfill(5)+'.txt'\n",
    "        #create \"artificial\" wildcard path for recent data. For the station we're looking at right now.       \n",
    "        recpath_temp = './ftp-cdc.dwd.de/pub/CDC/observations_germany/climate/hourly/'\n",
    "        recpath_temp += parametertype+'/recent/produkt*'\n",
    "        recpath_temp += str(station_number).zfill(5)+'.txt'   \n",
    "        \n",
    "        \n",
    "    #\"solar\" data are not divided into hist an recent - we need to introduce an exception for that case\n",
    "    elif parametertype == 'solar':\n",
    "        histpath_temp = './ftp-cdc.dwd.de/pub/CDC/observations_germany/climate/hourly/solar/produkt*'\n",
    "        histpath_temp += str(station_number).zfill(5)+'.txt'\n",
    "        recpath_temp = ''\n",
    "    \n",
    "    \n",
    "    #check if that path actually exists. Globglob checks if the histpath file actually exists.\n",
    "    if len(glob.glob(histpath_temp)) != 0:\n",
    "        #if file exists, save the path as a string to \"histpath\" variable.\n",
    "        histpath = glob.glob(histpath_temp)[0]\n",
    "        hist_ = pd.read_table(histpath, sep=\";\", low_memory=False)\n",
    "        #is_hist = True\n",
    "\n",
    "    else:\n",
    "        #is_hist=False\n",
    "        hist_ = []\n",
    "\n",
    "        #check if recent data exists\n",
    "    if len(glob.glob(recpath_temp)) != 0:\n",
    "        recpath = glob.glob(recpath_temp)[0]\n",
    "        rec_ = pd.read_table(recpath, sep=\";\", low_memory=False)\n",
    "        #is_rec = True\n",
    "\n",
    "    else:\n",
    "        #is_rec = False\n",
    "        rec_ = []\n",
    "    return (hist_,rec_)\n",
    "\n",
    "#(a,b) = get_hist_and_rec_hourly(3987,'solar')\n",
    "#print(b)\n",
    "        \n",
    "def merge_hist_rec_hourly(hist,rec,stationnumber, parametertype):    \n",
    "    if len(hist) != 0:\n",
    "        hist.columns = name_param_dict[parametertype]\n",
    "        hist = hist.ix[:len(hist)-2] #cut last line - it's empty\n",
    "        last_date = hist.Date[len(hist)-1] #extract last date of historical data\n",
    "        if len(rec) == 0:\n",
    "            complete_data = hist\n",
    "\n",
    "    if len(rec) != 0:\n",
    "        rec.columns = name_param_dict[parametertype]\n",
    "\n",
    "        if len(hist) != 0:\n",
    "            try:\n",
    "                rec_starting_idx = (rec.loc[rec['Date'] == last_date].index.tolist()[0])+1\n",
    "            #if hist and rec do not overlap, we don't want to cut rec.\n",
    "            except IndexError:\n",
    "                rec_starting_idx=1\n",
    "            #Assign rec_cut: it's the original rec, but starting from index specified above.\n",
    "            rec_cut = rec.ix[rec_starting_idx:]\n",
    "            combined = pd.concat([hist,rec_cut],ignore_index=True)\n",
    "            complete_data = combined\n",
    "\n",
    "        else:\n",
    "            complete_data = rec\n",
    "\n",
    "\n",
    "    complete_data = complete_data.replace(-999, np.nan, regex=True)\n",
    "    #note: this replaces existing files.   \n",
    "    complete_data.to_csv('./hourly_data/'+parametertype+\"_\"+str(stationnumber)+\".csv\")\n",
    "\n",
    "    \n",
    "start = timeit.default_timer()\n",
    "\n",
    "for params in foldernames:\n",
    "    for stationnumber in range(0,200):\n",
    "        (hist_out, rec_out) = get_hist_and_rec_hourly(stationnumber,params)\n",
    "        if (len(hist_out) != 0) or (len(rec_out) != 0):\n",
    "            merge_hist_rec_hourly(hist_out,rec_out, stationnumber, params)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "\n",
    "print (stop - start) \n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hm?\n",
    "the first what file?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
